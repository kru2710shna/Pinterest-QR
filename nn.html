<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Neural Network from Scratch | Krushna Thakkar</title>
  <link rel="stylesheet" href="nn.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>

  <!-- Back Button -->
  <a href="index.html" class="back-btn">← Back</a>

  <!-- Header -->
  <header class="project-header">
    <h1>🧠 Neural Network From Scratch</h1>
    <p class="subtitle">Custom implementation of ANN components — dense layers, activations, normalization</p>
  </header>

  <!-- Content Card -->
  <section class="project-grid">
    <div class="project-card">
      <img src="assets/self_nn.png" alt="Neural Network from Scratch" class="project-img">
      <div class="project-info">
        <h2>Overview</h2>
        <p>
          This project builds neural networks from the ground up in Python without frameworks.
          It includes dense layers, activation functions like ReLU, and normalization techniques,
          making it educational and modular for experimentation.
        </p>

        <h2>Key Concepts</h2>
        <ul>
          <li>🔢 Dense (Fully Connected) Layers implemented manually.</li>
          <li>🧩 Activation functions: ReLU, (and optionally Softmax for output).</li>
          <li>📊 Normalization or scaling of layer outputs to stabilize training.</li>
          <li>🔎 Dataset examples (e.g. spiral data) to test networks.</li>
        </ul>

        <h2>Why Build from Scratch?</h2>
        <p>
          To deeply understand how neural networks work internally—gradient updates, weight initialization,
          and forward/backward propagation—rather than relying on high-level libraries.
        </p>

        <h2>GitHub</h2>
        <a href="https://github.com/kru2710shna/Self_Made_Nueral_Network" 
           target="_blank" class="github-link">🔗 View Repository</a>
      </div>
    </div>
  </section>

</body>
</html>
